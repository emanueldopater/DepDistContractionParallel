{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/7rx61d551r33w7h93r7sdv1c0000gn/T/ipykernel_49943/469417104.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_emb = pd.read_csv('airports_128_0_node2vec.emb', sep=' ', header=None)  \n",
    "airport_roles = pd.read_csv('airport_roles.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NODE_ID</th>\n",
       "      <th>PROMINENCY_LABEL</th>\n",
       "      <th>PROMINENCY_WEIGHT</th>\n",
       "      <th>PROMINENCY_PURITY</th>\n",
       "      <th>RECIPROCITY_LABEL</th>\n",
       "      <th>RECIPROCITY_WEIGHT</th>\n",
       "      <th>RECIPROCITY_PURITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>StronglyProminent</td>\n",
       "      <td>1,06140350877193</td>\n",
       "      <td>0,00931055709449061</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>StronglyProminent</td>\n",
       "      <td>0,746268656716418</td>\n",
       "      <td>0,00556916907997327</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>StronglyProminent</td>\n",
       "      <td>0,128</td>\n",
       "      <td>0,001024</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>StronglyProminent</td>\n",
       "      <td>2,98870056497175</td>\n",
       "      <td>0,0168853139263941</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>StronglyProminent</td>\n",
       "      <td>0,09</td>\n",
       "      <td>0,0009</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>458</td>\n",
       "      <td>NonProminent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>459</td>\n",
       "      <td>NonProminent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>StronglyReciprocal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>460</td>\n",
       "      <td>NonProminent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>NonProminent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>462</td>\n",
       "      <td>NonProminent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NonReciprocal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NODE_ID   PROMINENCY_LABEL  PROMINENCY_WEIGHT    PROMINENCY_PURITY  \\\n",
       "0          1  StronglyProminent   1,06140350877193  0,00931055709449061   \n",
       "1          2  StronglyProminent  0,746268656716418  0,00556916907997327   \n",
       "2          3  StronglyProminent              0,128             0,001024   \n",
       "3          4  StronglyProminent   2,98870056497175   0,0168853139263941   \n",
       "4          5  StronglyProminent               0,09               0,0009   \n",
       "..       ...                ...                ...                  ...   \n",
       "457      458       NonProminent                  0                    0   \n",
       "458      459       NonProminent                  0                    0   \n",
       "459      460       NonProminent                  0                    0   \n",
       "460      461       NonProminent                  0                    0   \n",
       "461      462       NonProminent                  0                    0   \n",
       "\n",
       "      RECIPROCITY_LABEL RECIPROCITY_WEIGHT RECIPROCITY_PURITY  \n",
       "0         NonReciprocal                  0                  0  \n",
       "1         NonReciprocal                  0                  0  \n",
       "2         NonReciprocal                  0                  0  \n",
       "3         NonReciprocal                  0                  0  \n",
       "4         NonReciprocal                  0                  0  \n",
       "..                  ...                ...                ...  \n",
       "457       NonReciprocal                  0                  0  \n",
       "458  StronglyReciprocal                  1                  1  \n",
       "459       NonReciprocal                  0                  0  \n",
       "460       NonReciprocal                  0                  0  \n",
       "461       NonReciprocal                  0                  0  \n",
       "\n",
       "[462 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_emb_with_role = airport_emb.merge(airport_roles[['NODE_ID','PROMINENCY_LABEL']], left_on=0, right_on='NODE_ID')\n",
    "airport_emb_with_role = airport_emb_with_role.drop(columns=['NODE_ID', 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>PROMINENCY_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.063933</td>\n",
       "      <td>0.054588</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.050556</td>\n",
       "      <td>0.160908</td>\n",
       "      <td>-0.010950</td>\n",
       "      <td>-0.161459</td>\n",
       "      <td>-0.084790</td>\n",
       "      <td>0.235999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111348</td>\n",
       "      <td>0.076122</td>\n",
       "      <td>0.064705</td>\n",
       "      <td>0.144867</td>\n",
       "      <td>-0.154470</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>-0.034051</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.015871</td>\n",
       "      <td>StronglyProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068996</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.095545</td>\n",
       "      <td>0.125746</td>\n",
       "      <td>0.059112</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>-0.034523</td>\n",
       "      <td>-0.086110</td>\n",
       "      <td>0.272053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084844</td>\n",
       "      <td>0.057168</td>\n",
       "      <td>-0.059127</td>\n",
       "      <td>0.048876</td>\n",
       "      <td>-0.039810</td>\n",
       "      <td>0.113723</td>\n",
       "      <td>-0.054531</td>\n",
       "      <td>-0.032524</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>StronglyProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033917</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.050117</td>\n",
       "      <td>0.048434</td>\n",
       "      <td>0.238322</td>\n",
       "      <td>0.048428</td>\n",
       "      <td>0.189923</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>0.231640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>-0.114497</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>-0.041870</td>\n",
       "      <td>-0.074020</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.161038</td>\n",
       "      <td>-0.161156</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>StronglyProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184043</td>\n",
       "      <td>-0.277091</td>\n",
       "      <td>-0.020888</td>\n",
       "      <td>-0.013034</td>\n",
       "      <td>-0.031497</td>\n",
       "      <td>0.121122</td>\n",
       "      <td>-0.130410</td>\n",
       "      <td>-0.177157</td>\n",
       "      <td>-0.027745</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078421</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>-0.194771</td>\n",
       "      <td>-0.108847</td>\n",
       "      <td>0.160198</td>\n",
       "      <td>-0.178540</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>-0.117237</td>\n",
       "      <td>0.040194</td>\n",
       "      <td>StronglyProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.067894</td>\n",
       "      <td>0.071175</td>\n",
       "      <td>0.180074</td>\n",
       "      <td>0.113855</td>\n",
       "      <td>0.218414</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>-0.144999</td>\n",
       "      <td>-0.049340</td>\n",
       "      <td>-0.011345</td>\n",
       "      <td>0.231888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053104</td>\n",
       "      <td>-0.212758</td>\n",
       "      <td>0.260246</td>\n",
       "      <td>0.184051</td>\n",
       "      <td>-0.171183</td>\n",
       "      <td>-0.170551</td>\n",
       "      <td>-0.191680</td>\n",
       "      <td>-0.125550</td>\n",
       "      <td>-0.195252</td>\n",
       "      <td>StronglyProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.035888</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.177531</td>\n",
       "      <td>0.078997</td>\n",
       "      <td>0.138309</td>\n",
       "      <td>-0.242006</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.022751</td>\n",
       "      <td>0.367110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105111</td>\n",
       "      <td>0.078644</td>\n",
       "      <td>0.091999</td>\n",
       "      <td>-0.063185</td>\n",
       "      <td>-0.172420</td>\n",
       "      <td>-0.040271</td>\n",
       "      <td>0.051851</td>\n",
       "      <td>0.127846</td>\n",
       "      <td>-0.035507</td>\n",
       "      <td>NonProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.106969</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>-0.140961</td>\n",
       "      <td>0.105378</td>\n",
       "      <td>-0.022253</td>\n",
       "      <td>-0.009216</td>\n",
       "      <td>-0.102316</td>\n",
       "      <td>0.157874</td>\n",
       "      <td>-0.068019</td>\n",
       "      <td>0.201515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204088</td>\n",
       "      <td>-0.011691</td>\n",
       "      <td>0.117562</td>\n",
       "      <td>-0.077845</td>\n",
       "      <td>0.031232</td>\n",
       "      <td>0.129759</td>\n",
       "      <td>-0.100748</td>\n",
       "      <td>-0.148881</td>\n",
       "      <td>0.134401</td>\n",
       "      <td>NonProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-0.072001</td>\n",
       "      <td>0.260724</td>\n",
       "      <td>0.168977</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>0.292578</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>-0.209288</td>\n",
       "      <td>-0.067745</td>\n",
       "      <td>0.250530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182832</td>\n",
       "      <td>-0.163463</td>\n",
       "      <td>0.082283</td>\n",
       "      <td>0.277490</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>-0.140256</td>\n",
       "      <td>-0.088254</td>\n",
       "      <td>-0.072066</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>NonProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.225843</td>\n",
       "      <td>-0.038367</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.235046</td>\n",
       "      <td>-0.057757</td>\n",
       "      <td>0.217702</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>-0.069936</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.117218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>-0.234505</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>-0.064213</td>\n",
       "      <td>-0.216503</td>\n",
       "      <td>-0.240182</td>\n",
       "      <td>-0.037150</td>\n",
       "      <td>-0.162238</td>\n",
       "      <td>-0.044596</td>\n",
       "      <td>NonProminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.217669</td>\n",
       "      <td>0.064525</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.116326</td>\n",
       "      <td>-0.114051</td>\n",
       "      <td>0.097685</td>\n",
       "      <td>-0.134740</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>-0.031137</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025220</td>\n",
       "      <td>-0.060951</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.183748</td>\n",
       "      <td>-0.055775</td>\n",
       "      <td>-0.077127</td>\n",
       "      <td>0.124275</td>\n",
       "      <td>-0.105394</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>NonProminent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2         3         4         5         6         7  \\\n",
       "0    0.016058  0.063933  0.054588  0.040625  0.050556  0.160908 -0.010950   \n",
       "1    0.068996 -0.000120  0.068125  0.095545  0.125746  0.059112  0.010365   \n",
       "2    0.033917  0.163112  0.050117  0.048434  0.238322  0.048428  0.189923   \n",
       "3    0.184043 -0.277091 -0.020888 -0.013034 -0.031497  0.121122 -0.130410   \n",
       "4   -0.067894  0.071175  0.180074  0.113855  0.218414  0.151411 -0.144999   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "457  0.035888 -0.047917  0.219685  0.177531  0.078997  0.138309 -0.242006   \n",
       "458  0.106969 -0.270619 -0.140961  0.105378 -0.022253 -0.009216 -0.102316   \n",
       "459 -0.072001  0.260724  0.168977  0.160464  0.292578  0.001488  0.025246   \n",
       "460  0.225843 -0.038367  0.017112  0.235046 -0.057757  0.217702  0.031808   \n",
       "461  0.217669  0.064525  0.012090  0.116326 -0.114051  0.097685 -0.134740   \n",
       "\n",
       "            8         9        10  ...       120       121       122  \\\n",
       "0   -0.161459 -0.084790  0.235999  ... -0.111348  0.076122  0.064705   \n",
       "1   -0.034523 -0.086110  0.272053  ... -0.084844  0.057168 -0.059127   \n",
       "2   -0.008983 -0.128208  0.231640  ...  0.198227 -0.114497  0.091908   \n",
       "3   -0.177157 -0.027745  0.054277  ...  0.078421  0.032646 -0.194771   \n",
       "4   -0.049340 -0.011345  0.231888  ... -0.053104 -0.212758  0.260246   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "457 -0.036315 -0.022751  0.367110  ... -0.105111  0.078644  0.091999   \n",
       "458  0.157874 -0.068019  0.201515  ... -0.204088 -0.011691  0.117562   \n",
       "459 -0.209288 -0.067745  0.250530  ...  0.182832 -0.163463  0.082283   \n",
       "460 -0.069936  0.001426  0.117218  ...  0.018645 -0.234505  0.006695   \n",
       "461  0.033199 -0.031137  0.286800  ... -0.025220 -0.060951 -0.024402   \n",
       "\n",
       "          123       124       125       126       127       128  \\\n",
       "0    0.144867 -0.154470  0.234947 -0.034051  0.008744  0.015871   \n",
       "1    0.048876 -0.039810  0.113723 -0.054531 -0.032524  0.047727   \n",
       "2   -0.041870 -0.074020  0.212175  0.161038 -0.161156  0.013230   \n",
       "3   -0.108847  0.160198 -0.178540  0.019218 -0.117237  0.040194   \n",
       "4    0.184051 -0.171183 -0.170551 -0.191680 -0.125550 -0.195252   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "457 -0.063185 -0.172420 -0.040271  0.051851  0.127846 -0.035507   \n",
       "458 -0.077845  0.031232  0.129759 -0.100748 -0.148881  0.134401   \n",
       "459  0.277490  0.040657 -0.140256 -0.088254 -0.072066 -0.085449   \n",
       "460 -0.064213 -0.216503 -0.240182 -0.037150 -0.162238 -0.044596   \n",
       "461 -0.183748 -0.055775 -0.077127  0.124275 -0.105394  0.008066   \n",
       "\n",
       "      PROMINENCY_LABEL  \n",
       "0    StronglyProminent  \n",
       "1    StronglyProminent  \n",
       "2    StronglyProminent  \n",
       "3    StronglyProminent  \n",
       "4    StronglyProminent  \n",
       "..                 ...  \n",
       "457       NonProminent  \n",
       "458       NonProminent  \n",
       "459       NonProminent  \n",
       "460       NonProminent  \n",
       "461       NonProminent  \n",
       "\n",
       "[462 rows x 129 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_emb_with_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode PROMINENCY_LABEL to integers\n",
    "label_encoder = LabelEncoder()\n",
    "airport_emb_with_role['PROMINENCY_LABEL'] = label_encoder.fit_transform(airport_emb_with_role['PROMINENCY_LABEL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "X = airport_emb_with_role.drop(columns=['PROMINENCY_LABEL'])\n",
    "y = airport_emb_with_role['PROMINENCY_LABEL']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network from pytorch  to classify roles\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# create a dataloader from the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a neural network\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "model = Classifier()\n",
    "\n",
    "# create a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100..  Training Loss: 1.039..  Test Loss: 0.949..  Test Accuracy: 0.611\n",
      "Epoch: 2/100..  Training Loss: 0.893..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 3/100..  Training Loss: 0.891..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 4/100..  Training Loss: 0.894..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 5/100..  Training Loss: 0.901..  Test Loss: 0.943..  Test Accuracy: 0.609\n",
      "Epoch: 6/100..  Training Loss: 0.889..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 7/100..  Training Loss: 0.891..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 8/100..  Training Loss: 0.889..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 9/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 10/100..  Training Loss: 0.894..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 11/100..  Training Loss: 0.901..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 12/100..  Training Loss: 0.896..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 13/100..  Training Loss: 0.894..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 14/100..  Training Loss: 0.903..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 15/100..  Training Loss: 0.891..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 16/100..  Training Loss: 0.901..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 17/100..  Training Loss: 0.891..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 18/100..  Training Loss: 0.898..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 19/100..  Training Loss: 0.887..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 20/100..  Training Loss: 0.891..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 21/100..  Training Loss: 0.889..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 22/100..  Training Loss: 0.901..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 23/100..  Training Loss: 0.896..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 24/100..  Training Loss: 0.901..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 25/100..  Training Loss: 0.896..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 26/100..  Training Loss: 0.898..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 27/100..  Training Loss: 0.898..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 28/100..  Training Loss: 0.894..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 29/100..  Training Loss: 0.894..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 30/100..  Training Loss: 0.894..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 31/100..  Training Loss: 0.898..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 32/100..  Training Loss: 0.898..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 33/100..  Training Loss: 0.901..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 34/100..  Training Loss: 0.901..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 35/100..  Training Loss: 0.891..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 36/100..  Training Loss: 0.891..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 37/100..  Training Loss: 0.896..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 38/100..  Training Loss: 0.896..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 39/100..  Training Loss: 0.894..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 40/100..  Training Loss: 0.901..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 41/100..  Training Loss: 0.901..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 42/100..  Training Loss: 0.896..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 43/100..  Training Loss: 0.896..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 44/100..  Training Loss: 0.898..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 45/100..  Training Loss: 0.896..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 46/100..  Training Loss: 0.898..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 47/100..  Training Loss: 0.891..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 48/100..  Training Loss: 0.894..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 49/100..  Training Loss: 0.898..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 50/100..  Training Loss: 0.898..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 51/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 52/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 53/100..  Training Loss: 0.896..  Test Loss: 0.943..  Test Accuracy: 0.609\n",
      "Epoch: 54/100..  Training Loss: 0.891..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 55/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 56/100..  Training Loss: 0.894..  Test Loss: 0.935..  Test Accuracy: 0.616\n",
      "Epoch: 57/100..  Training Loss: 0.896..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 58/100..  Training Loss: 0.896..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 59/100..  Training Loss: 0.894..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 60/100..  Training Loss: 0.891..  Test Loss: 0.943..  Test Accuracy: 0.609\n",
      "Epoch: 61/100..  Training Loss: 0.896..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 62/100..  Training Loss: 0.889..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 63/100..  Training Loss: 0.896..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 64/100..  Training Loss: 0.898..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 65/100..  Training Loss: 0.894..  Test Loss: 0.934..  Test Accuracy: 0.617\n",
      "Epoch: 66/100..  Training Loss: 0.903..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 67/100..  Training Loss: 0.894..  Test Loss: 0.933..  Test Accuracy: 0.619\n",
      "Epoch: 68/100..  Training Loss: 0.898..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 69/100..  Training Loss: 0.898..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 70/100..  Training Loss: 0.891..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 71/100..  Training Loss: 0.894..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 72/100..  Training Loss: 0.901..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 73/100..  Training Loss: 0.898..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 74/100..  Training Loss: 0.896..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 75/100..  Training Loss: 0.896..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 76/100..  Training Loss: 0.889..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 77/100..  Training Loss: 0.894..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 78/100..  Training Loss: 0.894..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 79/100..  Training Loss: 0.905..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 80/100..  Training Loss: 0.898..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 81/100..  Training Loss: 0.905..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 82/100..  Training Loss: 0.901..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 83/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 84/100..  Training Loss: 0.894..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 85/100..  Training Loss: 0.891..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 86/100..  Training Loss: 0.887..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 87/100..  Training Loss: 0.889..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 88/100..  Training Loss: 0.891..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 89/100..  Training Loss: 0.898..  Test Loss: 0.942..  Test Accuracy: 0.610\n",
      "Epoch: 90/100..  Training Loss: 0.903..  Test Loss: 0.940..  Test Accuracy: 0.611\n",
      "Epoch: 91/100..  Training Loss: 0.896..  Test Loss: 0.939..  Test Accuracy: 0.612\n",
      "Epoch: 92/100..  Training Loss: 0.898..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 93/100..  Training Loss: 0.901..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 94/100..  Training Loss: 0.891..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 95/100..  Training Loss: 0.896..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 96/100..  Training Loss: 0.896..  Test Loss: 0.936..  Test Accuracy: 0.615\n",
      "Epoch: 97/100..  Training Loss: 0.891..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 98/100..  Training Loss: 0.896..  Test Loss: 0.938..  Test Accuracy: 0.613\n",
      "Epoch: 99/100..  Training Loss: 0.891..  Test Loss: 0.937..  Test Accuracy: 0.614\n",
      "Epoch: 100/100..  Training Loss: 0.894..  Test Loss: 0.939..  Test Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                log_ps = model(inputs)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.29032373428345%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    log_ps = model(X_test_tensor)\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == y_test_tensor.view(*top_class.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "    print(f'Accuracy: {accuracy.item()*100}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
